{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import tempfile\n",
    "import os\n",
    "import csv\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "num_datasets = 50\n",
    "\n",
    "search_results = api.dataset_list(search=\"\")\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for i, dataset in enumerate(search_results[:num_datasets]):\n",
    "    try:\n",
    "        dataset_name = dataset.ref\n",
    "        dataset_files = api.dataset_list_files(dataset_name).files\n",
    "\n",
    "        csv_file = next((file.name for file in dataset_files if file.name.endswith('.csv')), None)\n",
    "\n",
    "        if csv_file:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                api.dataset_download_file(dataset_name, csv_file, path=temp_dir, quiet=True)\n",
    "\n",
    "                file_path = os.path.join(temp_dir, csv_file)\n",
    "\n",
    "                with open(file_path, newline='') as csvfile:\n",
    "                    df_dict[f\"dataset_{i+1}\"] = list(csv.DictReader(csvfile))\n",
    "\n",
    "                print(f\"Loaded dataset {i+1}: {dataset_name}\")\n",
    "        else:\n",
    "            print(f\"No CSV file found in dataset {dataset_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {dataset_name}: {e}\")\n",
    "\n",
    "print(\"Datasets loaded into df_dict:\")\n",
    "print(df_dict.keys())\n"
   ],
   "id": "22d52f78de65e5ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80f48dbdf7b3be10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "d = {}\n",
    "for k, v in df_dict.items():\n",
    "    d[k] = df_dict[k]"
   ],
   "id": "3717591bcd19add4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "converted_data = {}\n",
    "for dataset_name, dataset_list in d.items():\n",
    "    if dataset_list:\n",
    "        columns = dataset_list[0].keys()\n",
    "        converted_data[dataset_name] = {\n",
    "            col: [item[col] for item in dataset_list]\n",
    "            for col in columns\n",
    "        }"
   ],
   "id": "1ebc896a18c2f712"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import pickle",
   "id": "e477b3efd741098e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('data.pkl', 'wb') as file:  # Note: 'wb' for binary write\n",
    "    pickle.dump(converted_data, file)"
   ],
   "id": "a5a5d1beb2e27f07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db447ed1d84c86c4"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
